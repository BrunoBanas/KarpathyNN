{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating bigrams - two letter objects capturing what letter is likely to follow another\n",
    "adding a start and an end characters to capture what letter to start with and with which letter the word is likely to end the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b={}\n",
    "for w in words[:1]:\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    for ch1, ch2 in zip(chs,chs[:1]):\n",
    "        bigram = (ch1, ch2)\n",
    "        b[bigram] = b.get(bigram, 0) +1 # counting the number that a bigram occurs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "storing the bigram information in a 2D array (pytorch tensors) where the rows are the first character and the columns are the second character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27,27), dtype=torch.int32)\n",
    "\n",
    "chars = sorted(list(set(''.join(words ))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} #mapping of a character to an index\n",
    "stoi['.'] = 0 # replacing special end and start characters with a single . representing the space between words\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs,chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2] # index of the character - defining position in the array\n",
    "        N[ix1,ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = N.float() #convert to float cause we want to devide\n",
    "P /= P.sum(1, keepdim=True) # normalize occurences , in-place operation has potential to be faster - doesnt create more memory under-the-hood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nieran.\n",
      "cele.\n",
      "brolyah.\n",
      "ananei.\n",
      "egin.\n",
      "baijeriesetengh.\n",
      "bebynnojarzakia.\n",
      "h.\n",
      "n.\n",
      "er.\n",
      "xlah.\n",
      "assaiavees.\n",
      "be.\n",
      "drikyly.\n",
      "en.\n",
      "d.\n",
      "an.\n",
      "ja.\n",
      "ma.\n",
      "am.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147484647)\n",
    "for i in range(20):\n",
    "    ix = 0\n",
    "    out=[]\n",
    "    while True:\n",
    "        p=P[ix]\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item() #obtianing normal distribuition from probabilities \n",
    "        out.append(itos[ix])\n",
    "        if ix ==0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when printing the probabilities of bigrams in real words we want the total probability according to a well trained model to be close to 1 (maximize the likelyhood). we can use log(likelyhood) for convenience - total prob = product of all probabilities which will be a tiny number for many words - log_likelyhood is a more natural number. \n",
    "negative log likelyhood - good loss function minimum is 0. use the average -."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.454094171524048\n"
     ]
    }
   ],
   "source": [
    "log_likelyhood = 0.0\n",
    "n=0\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs,chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2] \n",
    "        prob = P[ix1,ix2]\n",
    "        log_likelyhood += torch.log(prob)\n",
    "        n+=1\n",
    "nll = -log_likelyhood\n",
    "print(f'{nll/n}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
