{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram probability approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating bigrams - two letter objects capturing what letter is likely to follow another\n",
    "adding a start and an end characters to capture what letter to start with and with which letter the word is likely to end the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "b={}\n",
    "for w in words[:1]:\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    for ch1, ch2 in zip(chs,chs[:1]):\n",
    "        bigram = (ch1, ch2)\n",
    "        b[bigram] = b.get(bigram, 0) +1 # counting the number that a bigram occurs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "storing the bigram information in a 2D array (pytorch tensors) where the rows are the first character and the columns are the second character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27,27), dtype=torch.int32)\n",
    "\n",
    "chars = sorted(list(set(''.join(words ))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} #mapping of a character to an index\n",
    "stoi['.'] = 0 # replacing special end and start characters with a single . representing the space between words\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs,chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2] # index of the character - defining position in the array\n",
    "        N[ix1,ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N+1).float() #convert to float cause we want to devide\n",
    "#(N+1) is model smoothing give all possible bigrams at least 1 count removing infinities in log_likelyhood\n",
    "P /= P.sum(1, keepdim=True) # normalize occurences , in-place operation has potential to be faster - doesnt create more memory under-the-hood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nieran.\n",
      "cele.\n",
      "brolyah.\n",
      "ananei.\n",
      "egin.\n",
      "baijeriesetengh.\n",
      "bebynnojarzakia.\n",
      "h.\n",
      "n.\n",
      "er.\n",
      "xlah.\n",
      "assaiavees.\n",
      "be.\n",
      "drikyly.\n",
      "en.\n",
      "d.\n",
      "an.\n",
      "ja.\n",
      "ma.\n",
      "am.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147484647)\n",
    "for i in range(20):\n",
    "    ix = 0\n",
    "    out=[]\n",
    "    while True:\n",
    "        p=P[ix]\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item() #obtianing normal distribuition from probabilities \n",
    "        out.append(itos[ix])\n",
    "        if ix ==0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when printing the probabilities of bigrams in real words we want the total probability according to a well trained model to be close to 1 (maximize the likelyhood). we can use log(likelyhood) for convenience - total prob = product of all probabilities which will be a tiny number for many words - log_likelyhood is a more natural number. \n",
    "negative log likelyhood - good loss function minimum is 0. use the average -."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4543561935424805\n"
     ]
    }
   ],
   "source": [
    "log_likelyhood = 0.0\n",
    "n=0\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs,chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2] \n",
    "        prob = P[ix1,ix2]\n",
    "        log_likelyhood += torch.log(prob)\n",
    "        n+=1\n",
    "nll = -log_likelyhood\n",
    "print(f'{nll/n}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "N = torch.zeros((27,27), dtype=torch.int32)\n",
    "\n",
    "chars = sorted(list(set(''.join(words ))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} #mapping of a character to an index\n",
    "stoi['.'] = 0 # replacing special end and start characters with a single . representing the space between words\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [],[]\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs,chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2] # index of the character - defining position in the array\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs) # input\n",
    "ys = torch.tensor(ys) # output - second letter\n",
    "num = xs.nelement()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "right now, xs and ys are tensors of integer index values 0-27. these cant be used as input for NN - use one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "W = torch.randn((27,27), requires_grad=True) #random weights - makes 27 neurons, after multiplying with xsoh\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 27])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsoh = F.one_hot(xs, num_classes=27).float()\n",
    "xsoh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8248064517974854\n",
      "3.7358288764953613\n",
      "3.6533102989196777\n",
      "3.576688051223755\n",
      "3.5056183338165283\n",
      "3.43988299369812\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#backward pass\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     W\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward() \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#update\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     W\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m W\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[1;32m    268\u001b[0m     tensors,\n\u001b[1;32m    269\u001b[0m     grad_tensors_,\n\u001b[1;32m    270\u001b[0m     retain_graph,\n\u001b[1;32m    271\u001b[0m     create_graph,\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for n in range(10):\n",
    "#forward pass    \n",
    "    xsoh = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xsoh @ W #matrix multiplication - activation of the weights at the positions of xs, equivalent to log-counts\n",
    "    count = logits.exp() # exponantiate the tensor element-wise giving an positive value for all, eqiuvalent to count\n",
    "    prob = count / count.sum(1, keepdim=True) # normalize - SOFTMAX\n",
    "    #prob[0] will give a vector of probabilities representing what is likely to follow the first letter\n",
    "    #all operations are easily differentaible so also backpropagable \n",
    "    loss = -prob[torch.arange(num), ys].log().mean() #average nll used as loss, taking the probabilities of the next letter at the correct indecies for which we are training. When we input a word which we want to use as training data, the probabilities of interest are of characters within the word - these train the weights associated with the word\n",
    "    print(loss.item())\n",
    "\n",
    "#backward pass\n",
    "    W.grad = None\n",
    "    loss.backward() \n",
    "\n",
    "#update\n",
    "\n",
    "    W.data += -10 * W.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n",
    "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n",
    "E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n",
    "E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\n",
    "E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?\n",
    "E06: meta-exercise! Think of a fun/interesting exercise and complete it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs1, xs2, ys = [],[], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs,chs[1:],chs[2:]):\n",
    "        ix1 = stoi[ch1] # index of the character - defining position in the array\n",
    "        ix2 = stoi[ch2] \n",
    "        ix3 = stoi[ch3]\n",
    "        # Do I need to stop here if ix3 == 0? Does zip remove rows where entries are empty?\n",
    "        xs1.append(ix1)\n",
    "        xs2.append(ix2)\n",
    "        ys.append(ix3)\n",
    "\n",
    "\n",
    "xs1 = torch.tensor(xs1) # input\n",
    "xs2 = torch.tensor(xs2) # input\n",
    "xs = torch.stack((xs1, xs2), dim=1)\n",
    "ys = torch.tensor(ys) # output - third letter\n",
    "num = xs.nelement()\n",
    "num = int(num/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "W = torch.randn((27,27,27), requires_grad=True) #random weights - makes 27 neurons, after multiplying with xsoh\n",
    "\n",
    "# Convert (row, col) to a single index in a flattened 27x27 matrix\n",
    "flat_index = xs[:, 0] * 27 + xs[:, 1]\n",
    "\n",
    "# One-hot encode in a flattened space\n",
    "one_hot_flat = F.one_hot(flat_index, num_classes=27 * 27)\n",
    "\n",
    "# Reshape back to 27x27\n",
    "one_hot_matrix = one_hot_flat.view(len(xs),27, 27).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.39247727394104\n",
      "2.3920223712921143\n",
      "2.391569137573242\n",
      "2.3911173343658447\n",
      "2.3906667232513428\n",
      "2.3902175426483154\n",
      "2.3897695541381836\n",
      "2.3893229961395264\n",
      "2.3888776302337646\n",
      "2.3884339332580566\n",
      "2.387991189956665\n",
      "2.387550115585327\n",
      "2.3871099948883057\n",
      "2.386671543121338\n",
      "2.3862340450286865\n",
      "2.3857979774475098\n",
      "2.3853628635406494\n",
      "2.384929656982422\n",
      "2.3844974040985107\n",
      "2.384066343307495\n",
      "2.383636474609375\n",
      "2.3832080364227295\n",
      "2.3827807903289795\n",
      "2.382354736328125\n",
      "2.381930112838745\n",
      "2.3815064430236816\n",
      "2.3810842037200928\n",
      "2.3806631565093994\n",
      "2.3802430629730225\n",
      "2.379824638366699\n",
      "2.3794071674346924\n",
      "2.378990888595581\n",
      "2.3785758018493652\n",
      "2.378161668777466\n",
      "2.377748966217041\n",
      "2.3773372173309326\n",
      "2.376926898956299\n",
      "2.3765175342559814\n",
      "2.3761096000671387\n",
      "2.3757026195526123\n",
      "2.3752965927124023\n",
      "2.374892234802246\n",
      "2.374488592147827\n",
      "2.374086380004883\n",
      "2.373685121536255\n",
      "2.3732848167419434\n",
      "2.3728857040405273\n",
      "2.372487783432007\n",
      "2.372091054916382\n",
      "2.3716952800750732\n",
      "2.37130069732666\n",
      "2.3709070682525635\n",
      "2.3705146312713623\n",
      "2.3701233863830566\n",
      "2.3697328567504883\n",
      "2.3693437576293945\n",
      "2.368955612182617\n",
      "2.3685684204101562\n",
      "2.368182420730591\n",
      "2.367797613143921\n",
      "2.3674135208129883\n",
      "2.3670308589935303\n",
      "2.3666489124298096\n",
      "2.3662681579589844\n",
      "2.3658881187438965\n",
      "2.365509271621704\n",
      "2.3651316165924072\n",
      "2.3647549152374268\n",
      "2.364379644393921\n",
      "2.3640048503875732\n",
      "2.363631010055542\n",
      "2.363258123397827\n",
      "2.362886428833008\n",
      "2.362515926361084\n",
      "2.3621463775634766\n",
      "2.3617773056030273\n",
      "2.3614096641540527\n",
      "2.3610429763793945\n",
      "2.3606770038604736\n",
      "2.360311985015869\n",
      "2.35994815826416\n",
      "2.3595855236053467\n",
      "2.3592233657836914\n",
      "2.3588624000549316\n",
      "2.3585023880004883\n",
      "2.3581433296203613\n",
      "2.357785224914551\n",
      "2.3574278354644775\n",
      "2.3570716381073\n",
      "2.3567161560058594\n",
      "2.3563616275787354\n",
      "2.3560078144073486\n",
      "2.3556556701660156\n",
      "2.3553037643432617\n",
      "2.354952812194824\n",
      "2.3546030521392822\n",
      "2.3542540073394775\n",
      "2.35390567779541\n",
      "2.3535585403442383\n",
      "2.353212594985962\n",
      "2.3528666496276855\n",
      "2.352522134780884\n",
      "2.3521785736083984\n",
      "2.3518359661102295\n",
      "2.3514938354492188\n",
      "2.3511528968811035\n",
      "2.3508126735687256\n",
      "2.350473165512085\n",
      "2.35013484954834\n",
      "2.349797487258911\n",
      "2.3494603633880615\n",
      "2.3491244316101074\n",
      "2.3487894535064697\n",
      "2.3484551906585693\n",
      "2.3481216430664062\n",
      "2.3477890491485596\n",
      "2.3474574089050293\n",
      "2.3471267223358154\n",
      "2.3467965126037598\n",
      "2.3464670181274414\n",
      "2.3461389541625977\n",
      "2.345811128616333\n",
      "2.3454842567443848\n",
      "2.345158576965332\n",
      "2.3448331356048584\n",
      "2.3445088863372803\n",
      "2.3441851139068604\n",
      "2.343862295150757\n",
      "2.3435401916503906\n",
      "2.34321928024292\n",
      "2.3428988456726074\n",
      "2.3425791263580322\n",
      "2.3422603607177734\n",
      "2.341942310333252\n",
      "2.3416247367858887\n",
      "2.341308355331421\n",
      "2.3409926891326904\n",
      "2.340677499771118\n",
      "2.3403632640838623\n",
      "2.3400497436523438\n",
      "2.3397371768951416\n",
      "2.3394250869750977\n",
      "2.33911395072937\n",
      "2.338803291320801\n",
      "2.338493824005127\n",
      "2.3381848335266113\n",
      "2.337876558303833\n",
      "2.337568998336792\n",
      "2.3372621536254883\n",
      "2.336956262588501\n",
      "2.336651086807251\n",
      "2.336346387863159\n",
      "2.336042642593384\n",
      "2.3357396125793457\n",
      "2.335437059402466\n",
      "2.3351354598999023\n",
      "2.334834337234497\n",
      "2.334534168243408\n",
      "2.3342347145080566\n",
      "2.3339357376098633\n",
      "2.3336377143859863\n",
      "2.3333401679992676\n",
      "2.3330435752868652\n",
      "2.332747220993042\n",
      "2.3324520587921143\n",
      "2.3321573734283447\n",
      "2.3318634033203125\n",
      "2.3315699100494385\n",
      "2.331277370452881\n",
      "2.3309855461120605\n",
      "2.3306944370269775\n",
      "2.3304035663604736\n",
      "2.330113649368286\n",
      "2.329824447631836\n",
      "2.329535961151123\n",
      "2.3292479515075684\n",
      "2.32896089553833\n",
      "2.32867431640625\n",
      "2.328388214111328\n",
      "2.3281028270721436\n",
      "2.3278183937072754\n",
      "2.3275341987609863\n",
      "2.3272511959075928\n",
      "2.3269684314727783\n",
      "2.326686382293701\n",
      "2.3264050483703613\n",
      "2.326124429702759\n",
      "2.3258440494537354\n",
      "2.3255648612976074\n",
      "2.3252856731414795\n",
      "2.325007677078247\n",
      "2.324730157852173\n",
      "2.324453353881836\n",
      "2.3241770267486572\n",
      "2.3239011764526367\n",
      "2.3236260414123535\n",
      "2.3233516216278076\n",
      "2.32307767868042\n",
      "2.3228044509887695\n",
      "2.3225319385528564\n",
      "2.3222599029541016\n",
      "2.321988582611084\n",
      "2.3217179775238037\n",
      "2.3214473724365234\n",
      "2.3211777210235596\n",
      "2.320908784866333\n",
      "2.3206403255462646\n",
      "2.3203725814819336\n",
      "2.3201053142547607\n",
      "2.319838762283325\n",
      "2.319572687149048\n",
      "2.319307327270508\n",
      "2.319042444229126\n",
      "2.3187780380249023\n",
      "2.318514347076416\n",
      "2.318251371383667\n",
      "2.317988634109497\n",
      "2.3177268505096436\n",
      "2.317465305328369\n",
      "2.317204475402832\n",
      "2.316944122314453\n",
      "2.3166844844818115\n",
      "2.316425323486328\n",
      "2.316166877746582\n",
      "2.315908670425415\n",
      "2.3156516551971436\n",
      "2.315394639968872\n",
      "2.315138101577759\n",
      "2.314882278442383\n",
      "2.314626932144165\n",
      "2.3143720626831055\n",
      "2.3141181468963623\n",
      "2.3138647079467773\n",
      "2.3136115074157715\n",
      "2.313359022140503\n",
      "2.3131070137023926\n",
      "2.3128557205200195\n",
      "2.3126044273376465\n",
      "2.312354326248169\n",
      "2.3121044635772705\n",
      "2.3118550777435303\n",
      "2.3116064071655273\n",
      "2.3113582134246826\n",
      "2.311110496520996\n",
      "2.3108632564544678\n",
      "2.3106167316436768\n",
      "2.310370445251465\n",
      "2.3101248741149902\n",
      "2.309879779815674\n",
      "2.3096351623535156\n",
      "2.3093912601470947\n",
      "2.309147357940674\n",
      "2.3089044094085693\n",
      "2.308661699295044\n",
      "2.308419704437256\n",
      "2.308177947998047\n",
      "2.3079371452331543\n",
      "2.307696580886841\n",
      "2.3074567317962646\n",
      "2.3072171211242676\n",
      "2.3069779872894287\n",
      "2.306739568710327\n",
      "2.306501626968384\n",
      "2.3062639236450195\n",
      "2.3060266971588135\n",
      "2.3057899475097656\n",
      "2.305554151535034\n",
      "2.305318593978882\n",
      "2.3050835132598877\n",
      "2.3048486709594727\n",
      "2.304614543914795\n",
      "2.3043808937072754\n",
      "2.304147720336914\n",
      "2.303915023803711\n",
      "2.303682804107666\n",
      "2.3034510612487793\n",
      "2.30322003364563\n",
      "2.3029890060424805\n",
      "2.3027586936950684\n",
      "2.3025286197662354\n",
      "2.3022992610931396\n",
      "2.302070379257202\n",
      "2.3018414974212646\n",
      "2.3016138076782227\n",
      "2.3013858795166016\n",
      "2.301158905029297\n",
      "2.3009326457977295\n",
      "2.300706148147583\n",
      "2.300480365753174\n",
      "2.300255060195923\n",
      "2.30003023147583\n",
      "2.2998058795928955\n",
      "2.299582004547119\n",
      "2.299358367919922\n",
      "2.299135208129883\n",
      "2.298912763595581\n",
      "2.2986907958984375\n",
      "2.298468828201294\n",
      "2.2982473373413086\n",
      "2.2980265617370605\n"
     ]
    }
   ],
   "source": [
    "for n in range(300):\n",
    "#forward pass    \n",
    "    logits = torch.einsum('bij,ijk->bk', one_hot_matrix, W) #matrix multiplication - activation of the weights at the positions of xs, equivalent to log-counts\n",
    "    count = logits.exp() # exponantiate the tensor element-wise giving an positive value for all, eqiuvalent to count\n",
    "    prob = count / count.sum(1, keepdim=True) # normalize - SOFTMAX\n",
    "    #prob[0] will give a vector of probabilities representing what letter is likely to follow the first letter\n",
    "    #all operations are easily differentaible so also backpropagable \n",
    "    loss = -prob[torch.arange(num), ys].log().mean() #average negative log likelyhood used as loss, taking the probabilities of the next letter at the correct indecies for which we are training. When we input a word which we want to use as training data, the probabilities of interest are of characters within the word - these train the weights associated with the word\n",
    "    print(loss.item())\n",
    "\n",
    "#backward pass\n",
    "    W.grad = None\n",
    "    loss.backward() \n",
    "\n",
    "#update\n",
    "\n",
    "    W.data += -10 * W.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trionniwhskvglmagp.\n",
      "wxfjlanda.\n",
      "velie.\n",
      "brbaqptjevertumivqehyanna.\n",
      "bpzerey.\n",
      "te.\n",
      "cogzyla.\n",
      "zgcffnzjdkxarionwatvcbkbnwlqvpazaudmmrtnd.\n",
      "ami.\n",
      "fart.\n",
      "halrkdenianean.\n",
      "vayn.\n",
      "ya.\n",
      "gophmyhuziyluoe.\n",
      "essibxyden.\n",
      "za.\n",
      "chennafx.\n",
      "tyor.\n",
      "ton.\n",
      "upuoeigh.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator()#.manual_seed(2147484647)\n",
    "for i in range(20):\n",
    "    douplex = torch.tensor([[0,0]])\n",
    "    ix=0\n",
    "    out=[]\n",
    "    while True:\n",
    "        one_hot_matrix = F.one_hot(douplex[-1, 0] * 27 + douplex[-1, 1], num_classes=27 * 27).view(1,27, 27).float()\n",
    "        logits = torch.einsum('bij,ijk->bk', one_hot_matrix, W) \n",
    "        count = logits.exp()\n",
    "        prob = count / count.sum(1, keepdim=True)\n",
    "        ix = torch.multinomial(prob, num_samples=1, replacement=True, generator=g).item() #obtianing normal distribuition from probabilities \n",
    "        douplex[0, 0]=douplex[0, 1]\n",
    "        douplex[0, 1]=ix\n",
    "        #douplex.append(douplex[-1, :])\n",
    "        out.append(itos[ix])\n",
    "        if ix ==0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
